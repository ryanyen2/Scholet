{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import bs4\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "import copy\n",
    "from playwright.sync_api import sync_playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current crawler doesn't work in Google Colab as it uses playwright.sync_api. It can be rewritten as profile scraper and use GET requests, but on practice it is getting blocked very fast. Several ways was tried to escape blocking, however using playwright is one where it didn't get blocked at all.\n",
    "\n",
    "It also saves results to MongoDB, but it can be modified to support other ways.\n",
    "\n",
    "Currently we wait random amount of time between scraping, so we don't get blocked by server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleScholarScrapper:\n",
    "\n",
    "    USER_AGENT = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, db_name, collection_name):\n",
    "        self.client = MongoClient(\"127.0.0.1\", 27017)\n",
    "        self.db = self.client[db_name]\n",
    "        self.collection = self.db[collection_name]\n",
    "\n",
    "    def __get_html_content(self, url):\n",
    "        try:\n",
    "            with sync_playwright() as p:\n",
    "                browser = p.chromium.launch(headless=True)\n",
    "                page = browser.new_page()\n",
    "                page.set_extra_http_headers(self.USER_AGENT)\n",
    "                page.goto(url)\n",
    "                page.wait_for_load_state(\"load\")\n",
    "                page.wait_for_timeout(\n",
    "                    random.randint(5000, 10000)\n",
    "                )  # Random delay between 5 to 10 seconds\n",
    "                html_content = page.content()\n",
    "        except Exception as e:\n",
    "            print(f\"Error while fetching {url}: {e}\")\n",
    "            html_content = None\n",
    "\n",
    "        return html_content\n",
    "\n",
    "    def __get_abstract_from_gs(self, gs_url):\n",
    "        soup = bs4.BeautifulSoup(self.__get_html_content(gs_url), \"lxml\")\n",
    "        description = soup.find(\"div\", {\"class\": \"gsh_csp\"})\n",
    "\n",
    "        if not description:\n",
    "            description = soup.find(\"div\", {\"class\": \"gsh_small\"})\n",
    "        if not description:\n",
    "            description = soup.find(\"div\", {\"id\": \"gsc_oci_descr\"})\n",
    "\n",
    "        if description:\n",
    "            description = description.get_text()\n",
    "\n",
    "        time.sleep(random.randint(20, 35))\n",
    "        return str(description)\n",
    "\n",
    "    def process_publications(self, publications):\n",
    "        for publication_number, publication in enumerate(publications):\n",
    "            if publication[\"abstract\"] == \"None\":\n",
    "                abstract = self.__get_abstract_from_gs(publication[\"gs_url\"])\n",
    "                publications[publication_number][\"abstract\"] = abstract\n",
    "                # with open(\"all_data_filled_null_abstracts.json\", \"w\") as f:\n",
    "                #     json.dump(members_info, f)\n",
    "        return publications\n",
    "\n",
    "    def update_abstracts(self, members_info):\n",
    "        for member_info in tqdm(members_info):\n",
    "            print(member_info[\"authorID\"])\n",
    "            copy_member_info = copy.deepcopy(member_info)\n",
    "\n",
    "            copy_member_info[\"publications\"] = self.process_publications(\n",
    "                copy_member_info[\"publications\"]\n",
    "            )\n",
    "            copy_member_info[\"publications_pubdate\"] = self.process_publications(\n",
    "                copy_member_info[\"publications_pubdate\"]\n",
    "            )\n",
    "\n",
    "            self.collection.delete_one({\"authorID\": copy_member_info[\"authorID\"]})\n",
    "            self.collection.insert_one(copy_member_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of json file content:\n",
    "\n",
    "```\n",
    "[{\n",
    "  \"authorID\": \"T4wUsIMAAAAJ\",\n",
    "  \"publications\": [\n",
    "    {\n",
    "      \"title\": \"TARDBP mutations in individuals with sporadic and familial amyotrophic lateral sclerosis\",\n",
    "      \"gs_url\": \"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=T4wUsIMAAAAJ&citation_for_view=T4wUsIMAAAAJ:u5HHmVD_uO8C\",\n",
    "      \"abstract\": None,\n",
    "      \"doi\": \"https://doi.org/10.1038/ng.132\"\n",
    "    },\n",
    "  ],\n",
    "  \"publications_pubdate\": [\n",
    "    {\n",
    "      \"title\": \"TARDBP mutations in individuals with sporadic and familial amyotrophic lateral sclerosis\",\n",
    "      \"gs_url\": \"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=T4wUsIMAAAAJ&citation_for_view=T4wUsIMAAAAJ:u5HHmVD_uO8C\",\n",
    "      \"abstract\": None,\n",
    "      \"doi\": null\n",
    "    },\n",
    "  ]\n",
    "}\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = GoogleScholarScrapper(\"waterloo_ai\", \"abstracts\")\n",
    "\n",
    "with open(\"../gs_scrapped/waterloo_ai.abstracts_all.json\") as f:\n",
    "    members_info = json.load(f)\n",
    "\n",
    "scraper.update_abstracts(members_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
