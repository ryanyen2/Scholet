{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests, bs4, re, time, sys\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An advice is to run this in Google Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./AI_lab_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(object):\n",
    "    data_path = DATA_PATH\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        authorID,\n",
    "        name=None,\n",
    "        image_link=None,\n",
    "        interests=None,\n",
    "        citations=None,\n",
    "        hindex=None,\n",
    "        i10index=None,\n",
    "        citation_histogram=None,\n",
    "        coauthors=None,\n",
    "        publications=None,\n",
    "        publications_pubdate=None,\n",
    "        all_publications_retrieved=False,\n",
    "        all_publications_extracted=False,\n",
    "        cstart=0,\n",
    "        pagesize=20,  # 100 is Max page size in scholar\n",
    "        cookies=None,\n",
    "    ):\n",
    "\n",
    "        self.cstart = cstart\n",
    "        self.pagesize = pagesize\n",
    "        self.cookies = cookies\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        }\n",
    "\n",
    "        self.soup = None\n",
    "\n",
    "        self.authorID = authorID\n",
    "        self.name = name\n",
    "        self.image_link = image_link\n",
    "        self.interests = interests\n",
    "        self.citations = citations\n",
    "        self.hindex = hindex\n",
    "        self.i10index = i10index\n",
    "        self.citation_histogram = citation_histogram\n",
    "        self.coauthors = coauthors\n",
    "        self.publications = publications\n",
    "        self.publications_pubdate = publications_pubdate\n",
    "        self.all_publications_retrieved = all_publications_retrieved\n",
    "        self.all_publications_extracted = all_publications_extracted\n",
    "\n",
    "    def set_url(self):\n",
    "        return f\"https://scholar.google.com/citations?hl=en&user={self.authorID}&cstart={self.cstart}&pagesize={self.pagesize}\"\n",
    "\n",
    "    def set_url_sortby_pubdate(self):\n",
    "        return f\"https://scholar.google.com/citations?hl=en&user={self.authorID}&cstart={self.cstart}&pagesize={self.pagesize}&sortby=pubdate\"\n",
    "\n",
    "    def make_profile_request(self, custom_url=\"\"):\n",
    "        url = custom_url if len(custom_url) != 0 else self.set_url()\n",
    "        print(url)\n",
    "        time.sleep(30)\n",
    "        response = requests.request(\n",
    "            \"GET\", url, headers=self.headers, cookies=self.cookies\n",
    "        )\n",
    "        if response.status_code == 429:\n",
    "            raise Exception(\n",
    "                \"The server responded with Error 429. We have been detected. Wait before trying again.\"\n",
    "            )\n",
    "        self.cookies = response.cookies\n",
    "        return response\n",
    "\n",
    "    def get_soup(self):\n",
    "        self.soup = bs4.BeautifulSoup(\n",
    "            self.make_profile_request().content, \"html.parser\"\n",
    "        )\n",
    "\n",
    "    def make_coauthor_request(self):\n",
    "        url = self.set_url() + \"&view_op=list_colleagues\"\n",
    "        time.sleep(30)\n",
    "        response = requests.request(\"GET\", url, headers=self.headers)\n",
    "        if response.status_code == 429:\n",
    "            raise Exception(\n",
    "                \"The server responded with Error 429. We have been detected. Wait before trying again.\"\n",
    "            )\n",
    "        return response\n",
    "\n",
    "    def get_full_name(self):\n",
    "        name = self.soup.find(\"div\", {\"id\": \"gsc_prf_in\"})\n",
    "        if name:\n",
    "            self.name = name.get_text()\n",
    "\n",
    "    def get_image_link(self):\n",
    "        image = self.soup.find(\"img\", {\"id\": \"gsc_prf_pup-img\"})\n",
    "        if image:\n",
    "            self.image_link = image.get(\"src\")\n",
    "\n",
    "    def get_interests(self):\n",
    "        self.interests = list(\n",
    "            map(\n",
    "                lambda x: x.get_text(),\n",
    "                self.soup.find_all(\"a\", {\"class\": \"gsc_prf_inta\"}),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_citations_count(self, citation_info):\n",
    "        citation = citation_info.find(\n",
    "            \"a\", text=re.compile(\"Citations\"), attrs={\"class\": \"gsc_rsb_f\"}\n",
    "        )\n",
    "        if citation:\n",
    "            citation_value = citation.parent.parent.find_all(\n",
    "                \"td\", {\"class\": \"gsc_rsb_std\"}\n",
    "            )\n",
    "            if len(citation_value) > 0:\n",
    "                self.citations = int(citation_value[0].get_text())\n",
    "\n",
    "    def get_hindex(self, citation_info):\n",
    "        hindex = citation_info.find(\n",
    "            \"a\", text=re.compile(\"h-index\"), attrs={\"class\": \"gsc_rsb_f\"}\n",
    "        )\n",
    "        if hindex:\n",
    "            hindex_value = hindex.parent.parent.find_all(\"td\", {\"class\": \"gsc_rsb_std\"})\n",
    "            if len(hindex_value) > 0:\n",
    "                self.hindex = int(hindex_value[0].get_text())\n",
    "\n",
    "    def get_i10index(self, citation_info):\n",
    "        i10index = citation_info.find(\n",
    "            \"a\", text=re.compile(\"i10-index\"), attrs={\"class\": \"gsc_rsb_f\"}\n",
    "        )\n",
    "        if i10index:\n",
    "            i10index_value = i10index.parent.parent.find_all(\n",
    "                \"td\", {\"class\": \"gsc_rsb_std\"}\n",
    "            )\n",
    "            if len(i10index_value) > 0:\n",
    "                self.i10index = int(i10index_value[0].get_text())\n",
    "\n",
    "    def get_citation_metrics(self):\n",
    "        citation_info = self.soup.find(\"div\", {\"id\": \"gsc_rsb_cit\"})\n",
    "        if citation_info:\n",
    "            self.get_citations_count(citation_info)\n",
    "            self.get_hindex(citation_info)\n",
    "            self.get_i10index(citation_info)\n",
    "\n",
    "    def get_citation_histogram(self):\n",
    "        citation_hist = self.soup.find_all(\"div\", {\"class\": \"gsc_md_hist_w\"})\n",
    "        if citation_hist:\n",
    "            citation_hist = citation_hist[0]\n",
    "            citation_hist_time = list(\n",
    "                map(\n",
    "                    lambda x: x.get_text(),\n",
    "                    citation_hist.find_all(\"span\", {\"class\": \"gsc_g_t\"}),\n",
    "                )\n",
    "            )\n",
    "            citation_hist_cites = list(\n",
    "                map(\n",
    "                    lambda x: x.get_text(),\n",
    "                    citation_hist.find_all(\"a\", {\"class\": \"gsc_g_a\"}),\n",
    "                )\n",
    "            )\n",
    "            self.citation_histogram = list(zip(citation_hist_time, citation_hist_cites))\n",
    "\n",
    "    def get_coauthors(self):\n",
    "        coauthor_list = self.soup.find(\"div\", {\"id\": \"gsc_rsb_co\"})\n",
    "        if coauthor_list:\n",
    "            if coauthor_list.find(\"button\"):  # too many coauthors requires a request\n",
    "                coauthor_list = bs4.BeautifulSoup(\n",
    "                    self.make_coauthor_request().content, \"html.parser\"\n",
    "                ).find(\"div\", {\"id\": \"gsc_codb_content\"})\n",
    "                coauthor_list = coauthor_list.find_all(\"div\", {\"class\": \"gsc_ucoar\"})\n",
    "                coauthor_ids = list(\n",
    "                    map(lambda x: x.get(\"id\").split(\"-\")[-1], coauthor_list)\n",
    "                )\n",
    "                coauthor_names = list(\n",
    "                    map(lambda x: x.find(\"img\").get(\"alt\"), coauthor_list)\n",
    "                )\n",
    "                self.coauthors = list(zip(coauthor_ids, coauthor_names))\n",
    "            else:\n",
    "                coauthor_list = coauthor_list.find_all(\"img\")\n",
    "                coauthor_ids = list(\n",
    "                    map(lambda x: x.get(\"id\").split(\"-\")[1], coauthor_list)\n",
    "                )\n",
    "                coauthor_names = list(map(lambda x: x.get(\"alt\"), coauthor_list))\n",
    "                self.coauthors = list(zip(coauthor_ids, coauthor_names))\n",
    "\n",
    "    def extract_compact_publication(self, publication_element):\n",
    "        title = publication_element.find(\"a\", {\"class\": \"gsc_a_at\"}).get_text()\n",
    "        year = (\n",
    "            publication_element.find(\"td\", {\"class\": \"gsc_a_y\"})\n",
    "            .find(\"span\", {\"class\": \"gsc_a_hc\"})\n",
    "            .get_text()\n",
    "        )\n",
    "        if year:\n",
    "            year = int(year)\n",
    "        else:\n",
    "            year = None\n",
    "        url = (\n",
    "            publication_element.find(\"a\", {\"class\": \"gsc_a_at\"})\n",
    "            .get(\"href\")\n",
    "            .split(\"?\")[-1]\n",
    "        )\n",
    "        cited_by = publication_element.find(\"a\", {\"class\": \"gsc_a_ac\"}).get_text()\n",
    "        if cited_by:\n",
    "            cited_by = int(cited_by)\n",
    "        else:\n",
    "            cited_by = None\n",
    "        return {\"title\": title, \"year\": year, \"cited_by\": cited_by, \"url\": url}\n",
    "\n",
    "    def get_publications_list(self):\n",
    "        publication_list = []\n",
    "\n",
    "        items = self.soup.find_all(\"tr\", {\"class\": \"gsc_a_tr\"})\n",
    "        publication_list += items\n",
    "        self.publications = list(\n",
    "            map(lambda x: self.extract_compact_publication(x), publication_list)\n",
    "        )\n",
    "\n",
    "        publication_pubdate_list = []\n",
    "        url_sortby_pubdate = self.set_url_sortby_pubdate()\n",
    "\n",
    "        soup = bs4.BeautifulSoup(\n",
    "            self.make_profile_request(custom_url=url_sortby_pubdate).content,\n",
    "            \"html.parser\",\n",
    "        )\n",
    "        items = soup.find_all(\"tr\", {\"class\": \"gsc_a_tr\"})\n",
    "        publication_pubdate_list += items\n",
    "        self.publications_pubdate = list(\n",
    "            map(lambda x: self.extract_compact_publication(x), publication_pubdate_list)\n",
    "        )\n",
    "\n",
    "    def get_publications_detail(self):\n",
    "        unscraped_publications = filter(\n",
    "            lambda x: not x.get(\"detail_extracted\", False), self.publications\n",
    "        )\n",
    "        for publication in unscraped_publications:\n",
    "            successful = publication.scrape()\n",
    "            if not successful:\n",
    "                break\n",
    "        self.set_all_publications_extracted()\n",
    "\n",
    "    def set_all_publications_extracted(self):\n",
    "        checker = next(\n",
    "            filter(lambda x: not x.get(\"detail_extracted\", False), self.publications),\n",
    "            None,\n",
    "        )\n",
    "        if checker is None:\n",
    "            self.all_publications_extracted = True\n",
    "        else:\n",
    "            self.all_publications_extracted = False\n",
    "\n",
    "    def save_data(self):\n",
    "        data = self.export_json()\n",
    "        with open(f\"{self.data_path}/{self.authorID}.json\", \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "        if self.all_publications_extracted:\n",
    "            print(\"The data extraction was complete.\")\n",
    "            print(\n",
    "                f'The extracted information is saved into the \"{self.authorID}.json\" file in the selected data path.'\n",
    "            )\n",
    "        else:\n",
    "            print(\"The publication detail extraction was incomplete.\")\n",
    "            print(\n",
    "                f'The extracted information is saved into the \"{self.authorID}.json\" file in the selected data path.'\n",
    "            )\n",
    "            print(\n",
    "                \"We will continue to extract the detail of the remaining publications next time you try.\"\n",
    "            )\n",
    "\n",
    "    def export_json(self):\n",
    "        data = {\n",
    "            \"authorID\": self.authorID,\n",
    "            \"name\": self.name,\n",
    "            \"image_link\": self.image_link,\n",
    "            \"interests\": self.interests,\n",
    "            \"citations\": self.citations,\n",
    "            \"hindex\": self.hindex,\n",
    "            \"i10index\": self.i10index,\n",
    "            \"citation_histogram\": self.citation_histogram,\n",
    "            # 'coauthors': self.coauthors,\n",
    "            \"publications\": [],\n",
    "            \"all_publications_retrieved\": self.all_publications_retrieved,\n",
    "            \"all_publications_extracted\": self.all_publications_extracted,\n",
    "            \"cstart\": self.cstart,\n",
    "            \"pagesize\": self.pagesize,\n",
    "        }\n",
    "        # data['publications'] = list(map(lambda x: x.export_json(), self.publications))\n",
    "        data[\"publications\"] = self.publications\n",
    "        data[\"publications_pubdate\"] = self.publications_pubdate\n",
    "        return data\n",
    "\n",
    "    def scrape(self):\n",
    "        if not self.all_publications_retrieved:\n",
    "            self.get_soup()\n",
    "            self.get_full_name()\n",
    "            self.get_image_link()\n",
    "            self.get_interests()\n",
    "            self.get_citation_metrics()\n",
    "            self.get_citation_histogram()\n",
    "            # self.get_coauthors()\n",
    "            self.get_publications_list()\n",
    "        # if not self.all_publications_extracted:\n",
    "        #   self.get_publications_detail()\n",
    "        self.save_data()\n",
    "\n",
    "\n",
    "def create_author(authorID):\n",
    "    try:\n",
    "        with open(f\"{DATA_PATH}/{authorID}.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        publications_data = data.pop(\"publications\")\n",
    "        cstart = data.pop(\"cstart\")\n",
    "        pagesize = data.pop(\"pagesize\")\n",
    "\n",
    "        author = Author(**data)\n",
    "        author.get_soup()\n",
    "        author.cstart = cstart\n",
    "        author.pagesize = pagesize\n",
    "\n",
    "        author.publications = author.get_publications_list()\n",
    "\n",
    "        return author\n",
    "    except FileNotFoundError:\n",
    "        return Author(authorID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(f\"../raw_data/Waterloo_AI_Faculty_Email_List_edited.xlsx\")\n",
    "df.columns = [\n",
    "    \"email\",\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "    \"faculty\",\n",
    "    \"department\",\n",
    "    \"area_of_focus\",\n",
    "    \"gs_link\",\n",
    "]\n",
    "for gs_link in df[\"gs_link\"]:\n",
    "    if type(gs_link) is str:\n",
    "        AUTHOR_ID = gs_link.split(\"user=\")[-1].split(\"&\")[0]\n",
    "        print(AUTHOR_ID)\n",
    "        author_obj = create_author(AUTHOR_ID)\n",
    "        author_obj.scrape()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
